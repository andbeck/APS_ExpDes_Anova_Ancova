[["randomized-complete-block-design-rcb.html", "Chapter 6 Randomized Complete Block design (RCB) 6.1 An example of the RCBD 6.2 Analysing the CRBD", " Chapter 6 Randomized Complete Block design (RCB) Blocking allows us to reduce the experimental error. A block is a group of experimental units that are homogeneous in some sense – in the same place, or measured at the same time, or by the same person. So when constructing blocks we try and select experimental units that are homogeneous within blocks but units in different blocks may be dissimilar. Why block? When we use a completely randomised design, the location or timing of our treatment ‘plots’ (patches, incubators, locations in a 96 well plate) can generate heterogeneity in experimental error (variation). As the variance of the Experimental Error increases, confidence intervals get wider and the power of our analysis decreases - it’s harder to detect effects of our treatments against the background noise. Ideally we would like to use experimental units that are homogeneous so the experimental error will be small. Blocking does this. The simplest blocked design is the Randomized Complete Block design (RCB) We have one complete set of treatments in each block. Say you have g Treatments and r Blocks then the total number of experimental units is? In the first block, we randomly assign the g treatments to the n units; we do an independent randomization, assigning treatments to units in each of the other blocks. This is the RCB design. For example, consider the following matrix: the rows are the blocks, the letters the different treatments. In each block, each treatment is represented, but it is in a different location in the block (randomisation of the g treatments in the n units). The blocks are in a sequence - left to right - this could be different days, different locations or different positions on a hillside, for example representing an elevation or soil moisture gradient. The Blocks are designed to ‘capture’ that underlying source of variability and allow us to detect among treatment differences more effectively. ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;C&quot; &quot;C&quot; &quot;B&quot; &quot;A&quot; &quot;C&quot; ## [2,] &quot;B&quot; &quot;A&quot; &quot;C&quot; &quot;D&quot; &quot;D&quot; ## [3,] &quot;E&quot; &quot;B&quot; &quot;A&quot; &quot;E&quot; &quot;B&quot; ## [4,] &quot;D&quot; &quot;E&quot; &quot;D&quot; &quot;C&quot; &quot;A&quot; ## [5,] &quot;A&quot; &quot;D&quot; &quot;E&quot; &quot;B&quot; &quot;E&quot; Here is another picture of a block design that moves from just letters to something more literal. The blocks are arranged along a gradient, say along the side of a hill, so represent low and high elevation and associated soil moisture. The blocks capture this background variation. THEN, each treatment level (1-4) is allocated a random position in each block.  In the end, each treatment level is replicated across blocks (n = 6!). From: https://www.researchgate.net/publication/322369242_Randomized_Block_Design_probiotic_example/figures?lo=1 It is important to note that blocks exist at the time of the randomization of treatments to units. We cannot impose blocking structure on a completely randomized design after the fact; either the randomization was blocked or it was not. We use an RCB to increase the power and precision of an experiment by decreasing the error variance. This decrease in error variance is achieved by finding groups of units that are homogeneous (blocks) and, in effect, repeating the experiment independently in the different blocks. The RCB is an effective design when there is a single source of extraneous variation in the responses that we can identify ahead of time and use to partition the units into blocks. In short ALWAYS block your experiment, if you can. You can have spatial blocks, or temporal blocks where you repeat the experiment at different times, or block by batch. In general, any source of variation that you think may influence the response and which can be identified prior to the experiment is a candidate for blocking. 6.1 An example of the RCBD Lets modify our previous example to including blocking. Start another section with some ## and call it Blocking Example. Of course, if you want to start another script, you can, but make sure you include all of the library()’s again! #Randomised Complete Block Design # ensure allocation is the same set.seed(123) # define the treatments treat &lt;- c(&quot;Control&quot;,&quot;Herb1&quot;,&quot;Herb2&quot;,&quot;Placebo&quot;) # define the number of blocks Nblocks &lt;- 5 # consider this Total.units &lt;- Nblocks * length(treat) # build the design design &lt;- design.rcbd(treat, Nblocks, serie = 0)$book # look at it design ## plots block treat ## 1 11 1 Placebo ## 2 12 1 Herb2 ## 3 13 1 Control ## 4 14 1 Herb1 ## 5 21 2 Herb1 ## 6 22 2 Control ## 7 23 2 Placebo ## 8 24 2 Herb2 ## 9 31 3 Herb1 ## 10 32 3 Herb2 ## 11 33 3 Control ## 12 34 3 Placebo ## 13 41 4 Placebo ## 14 42 4 Herb1 ## 15 43 4 Control ## 16 44 4 Herb2 ## 17 51 5 Placebo ## 18 52 5 Herb2 ## 19 53 5 Control ## 20 54 5 Herb1 This is like the matrix above, but in tidy format! Excellent. This is how a block design looks in tidy-land. Now, lets generate some data again. # set seed again ... set.seed(123) # define the error - note how we use the variable Total.units to get the number of observations error &lt;- rnorm(Total.units, mean = 0, sd = 1) # is this more or less variation than before? # generate the observations # note that we are now generating larger differences (10 and 9) among treatments # e.g. Herb1 is 10 units larger than the control. design$obs &lt;- 20 + (design$treat==&quot;Herb1&quot;) * 10 + (design$treat == &quot;Herb2&quot;) * 9 + (design$treat == &quot;Placebo&quot;) * 1 + # note that we are defining variation among blocks here # block 1 is on average 10 units higher.... and block 5 is now 10 units lower... (design$block==1) * 10 - (design$block==5) * 10 + # now add the error variation error head(design, 10) ## plots block treat obs ## 1 11 1 Placebo 31.18707 ## 2 12 1 Herb2 40.99603 ## 3 13 1 Control 32.29937 ## 4 14 1 Herb1 41.12746 ## 5 21 2 Herb1 31.08111 ## 6 22 2 Control 20.56917 ## 7 23 2 Placebo 20.09290 ## 8 24 2 Herb2 29.02990 ## 9 31 3 Herb1 28.44097 ## 10 32 3 Herb2 29.29749 6.2 Analysing the CRBD I’ll leave it to you now to generate the following plot of the means ± standard errors. This requires thinking hard about the use of dplyr tools (group_by() and summarise()) and ggplot (adding more than one layer from two different sources of data - the summary data and the raw data). You need to make a sumDat object for the means and se’s. Then you need to plot the raw data, and overlay the mean±se info from the sumDat. Can you see the variation we generated between block 1 and 5? Block 2-4 are all similar…. Block 1 is 10 units more, and Block 5 is 10 units less. ## Rows: 20 ## Columns: 4 ## $ plots &lt;dbl&gt; 11, 12, 13, 14, 21, 22, 23, 24, 31, 32, 33, 34, 41, 42, 43, 44, … ## $ block &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5 ## $ treat &lt;fct&gt; Placebo, Herb2, Control, Herb1, Herb1, Control, Placebo, Herb2, … ## $ obs &lt;dbl&gt; 31.187070, 40.996029, 32.299370, 41.127458, 31.081114, 20.569166… 6.2.1 Building the model In order to understand what’s going on with blocking, and it’s importance, lets build the naive model that ignores block - treating this as a CRB - and the correct model, letting block absorb some of the variation. # models naive_model &lt;- lm(obs ~ treat, design) block_model &lt;- lm(obs ~ block + treat, design) # note the order is important # anova tables anova(naive_model) ## Analysis of Variance Table ## ## Response: obs ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 443.72 147.905 2.6668 0.08288 . ## Residuals 16 887.38 55.461 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(block_model) ## Analysis of Variance Table ## ## Response: obs ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## block 4 877.08 219.270 255.52 1.692e-11 *** ## treat 3 443.72 147.905 172.36 3.954e-10 *** ## Residuals 12 10.30 0.858 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The first important thing to focus on here is the difference in the Mean Sq Residual Errors - in the naive_model, it is \\(55.52\\). In the block_model, it is \\(0.94\\). The second important thing to notice is that haveing allocated variation to block in the block_model, and thus reducing the error variation, the treatment effect shifts from being insignificant to significant. 6.2.2 Are the estimates of the parameters what we expect? Lets check that the model is estimating differences as we might have expected. We can do this using the summary table. Let’s remember that, for example, the mean of Herb1 is expected to be 10 units higher than control with a yield of 20, and block 1 is supposed to be ~10 units higher than 2,3,4. summary(block_model) ## ## Call: ## lm(formula = obs ~ block + treat, data = design) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9087 -0.7237 0.0965 0.4872 1.2848 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.4053 0.5859 53.604 1.17e-15 *** ## block2 -11.2092 0.6550 -17.112 8.54e-10 *** ## block3 -12.1132 0.6550 -18.492 3.48e-10 *** ## block4 -11.3415 0.6550 -17.314 7.46e-10 *** ## block5 -20.8449 0.6550 -31.823 5.85e-13 *** ## treatHerb1 10.0485 0.5859 17.151 8.32e-10 *** ## treatHerb2 9.3436 0.5859 15.948 1.92e-09 *** ## treatPlacebo 0.5967 0.5859 1.018 0.329 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9264 on 12 degrees of freedom ## Multiple R-squared: 0.9923, Adjusted R-squared: 0.9878 ## F-statistic: 219.9 on 7 and 12 DF, p-value: 1.051e-11 In this table, the INTERCEPT is specifying the FIRST BLOCK and the CONTROL TREATMENT LEVEL - we know this because it’s these words that are missing from the rest of the table, and they are each the first alphanumerially in the list of blocks and treatments. The value of the control, block 1 is approximately 30! Which is 20+10, which is what we expected. The value of Herb1 is ~10 units higher than this (remember, the value 9.84 is the DIFFERENCE between the control and treatment) And the value of block 5 is reported as 20 unites lower than block 1 control. This too is correct because, as above, block 1 control is 10 units higher than the control mean (20+10) and block 5 is 10 units lower…. Make sure you get this logic! The take home message here is that these numbers from the model make complete sense with respect to the data that we simulated. Furthermore, controlling for the among block variation gave us more power to detect a treatment effect, something we would have missed had we not estimated the block source of variation. 6.2.3 Correct Standard Errors for a Figure When we made our initial plot above, we calculated the standard error based on all observations among blocks. However, the variation we really wish to represent is the variation after having controlled for the blocking effects. This means that the standard deviation we should probably use is of the error variance from the correct model: \\(0.94\\). The standard deviation is the \\(\\sqrt{Var}\\) and thus, our correct standard errors from the model are \\(\\sqrt{0.94}\\) There is a very nice plotting function in the package visreg that delivers these proper standard errors in a nice ggplot framework. It presents points that are the partial residuals (deviation from the mean for each replicate), lines depicting the means, and shaded area as a 95% confidence interval, calculated as 1.96*SE, where the SE is estimated from the model error variance (just above). Compare this to your first graph. visreg(block_model, &quot;treat&quot;, gg=TRUE)+ ylab(&quot;Yield&quot;) + xlab(&quot;Treatment&quot;) 6.2.4 Making inference: confidence intervals and contrasts We are now in a very strong position to make inference. Let’s start with a rule of thumb linked to the 95% confidence interval. If the CIs don’t overlap, they are different; if they do, they are not. This indicates that Cont and Placebo are not significantly different (95% confidence intervals overlap). Herb1 and Herb 2 are significantly different from these, but not each other. This is OK. But it’s not robust. Let’s revisit our post-hoc and a priori methods for evaluating differnces among treatments. We can apply a tukey test and calculate all pairwise differences. This is not a good idea, but let’s do it. # use agricolae HSD.test() tukey_out &lt;- HSD.test(block_model, &quot;treat&quot;, group = TRUE) tukey_out$groups ## obs groups ## Herb1 30.35200 a ## Herb2 29.64713 a ## Placebo 20.90022 b ## Control 20.30352 b This confirms our intuition and 95% Confidence Interval insights. But is it correct? Let’s make a formal test of one of the pairwise tests that looks obvious - between Herb1 and Herb2 # fit.contrast from gmodels package contrast &lt;- c(0,-1,1,0) fit.contrast(block_model, &quot;treat&quot;, contrast) ## Estimate Std. Error t value Pr(&gt;|t|) ## treat c=( 0 -1 1 0 ) -0.704871 0.5858793 -1.203099 0.2521374 ## attr(,&quot;class&quot;) ## [1] &quot;fit_contrast&quot; Amazing. The contrast defining a specific test provides a different answer than the post-hoc Tukey test and our guess based on the 95% CIs. Why is that? Which is right? Of course the contrast is the correct and most reliable result. While both fit.contrast and HSD.test both manage the model complexity and variance estimates properly, only the contrast reduces the probability of finding a significant difference by chance or failing to find one. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
